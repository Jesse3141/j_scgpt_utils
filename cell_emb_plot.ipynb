{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#create jupyter section header for navigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import torch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-04T15:23:22.304186711Z",
     "start_time": "2023-12-04T15:23:22.273980278Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "#import\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import torch\n",
    "import short_utils\n",
    "\n",
    "from anndata import AnnData\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from scgpt import logger\n",
    "from scgpt.data_collator import DataCollator\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import GeneVocab\n",
    "from scgpt.utils import load_pretrained\n",
    "\n",
    "import pyperclip\n",
    "\n",
    "\n",
    "\n",
    "PathLike = Union[str, os.PathLike]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch embed func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:14:41.527010020Z",
     "start_time": "2023-12-03T14:14:41.494945479Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch_cell_embeddings(\n",
    "    adata,\n",
    "    cell_embedding_mode: str = \"cls\",\n",
    "    model=None,\n",
    "    vocab=None,\n",
    "    max_length=1200,\n",
    "    batch_size=64,\n",
    "    model_configs=None,\n",
    "    gene_ids=None,\n",
    "    use_batch_labels=False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the cell embeddings for a batch of cells.\n",
    "\n",
    "    Args:\n",
    "        adata (AnnData): The AnnData object.\n",
    "        cell_embedding_mode (str): The mode to get the cell embeddings. Defaults to \"cls\".\n",
    "        model (TransformerModel, optional): The model. Defaults to None.\n",
    "        vocab (GeneVocab, optional): The vocabulary. Defaults to None.\n",
    "        max_length (int): The maximum length of the input sequence. Defaults to 1200.\n",
    "        batch_size (int): The batch size for inference. Defaults to 64.\n",
    "        model_configs (dict, optional): The model configurations. Defaults to None.\n",
    "        gene_ids (np.ndarray, optional): The gene vocabulary ids. Defaults to None.\n",
    "        use_batch_labels (bool): Whether to use batch labels. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The cell embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    count_matrix = adata.X\n",
    "    count_matrix = (\n",
    "        count_matrix if isinstance(count_matrix, np.ndarray) else count_matrix.A\n",
    "    )\n",
    "    print(\"loaded count matrix\")\n",
    "\n",
    "    # gene vocabulary ids\n",
    "    if gene_ids is None:\n",
    "        gene_ids = np.array(adata.var[\"id_in_vocab\"])\n",
    "        assert np.all(gene_ids >= 0)\n",
    "\n",
    "    if use_batch_labels:\n",
    "        batch_ids = np.array(adata.obs[\"batch_id\"].tolist())\n",
    "\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, count_matrix, gene_ids, batch_ids=None):\n",
    "            self.count_matrix = count_matrix\n",
    "            self.gene_ids = gene_ids\n",
    "            self.batch_ids = batch_ids\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.count_matrix)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            row = self.count_matrix[idx]\n",
    "            nonzero_idx = np.nonzero(row)[0]\n",
    "            values = row[nonzero_idx]\n",
    "            genes = self.gene_ids[nonzero_idx]\n",
    "            # append <cls> token at the beginning\n",
    "            genes = np.insert(genes, 0, vocab[\"<cls>\"])\n",
    "            values = np.insert(values, 0, model_configs[\"pad_value\"])\n",
    "            genes = torch.from_numpy(genes).long()\n",
    "            values = torch.from_numpy(values)\n",
    "            output = {\n",
    "                \"id\": idx,\n",
    "                \"genes\": genes,\n",
    "                \"expressions\": values,\n",
    "            }\n",
    "            if self.batch_ids is not None:\n",
    "                output[\"batch_labels\"] = self.batch_ids[idx]\n",
    "            return output\n",
    "\n",
    "    if cell_embedding_mode == \"cls\":\n",
    "        dataset = Dataset(\n",
    "            count_matrix, gene_ids, batch_ids if use_batch_labels else None\n",
    "        )\n",
    "        print(\"created dataset\")\n",
    "        collator = DataCollator(\n",
    "            do_padding=True,\n",
    "            pad_token_id=vocab[model_configs[\"pad_token\"]],\n",
    "            pad_value=model_configs[\"pad_value\"],\n",
    "            do_mlm=False,\n",
    "            do_binning=True,\n",
    "            max_length=max_length,\n",
    "            sampling=True,\n",
    "            keep_first_n_tokens=1,\n",
    "        )\n",
    "        print(\"created collator\")\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SequentialSampler(dataset),\n",
    "            collate_fn=collator,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(\"created data loader\")\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "        print(\"created device\")\n",
    "        cell_embeddings = np.zeros(\n",
    "            (len(dataset), model_configs[\"embsize\"]), dtype=np.float32\n",
    "        )\n",
    "        print(\"created intial cell embeddings\")\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n",
    "            count = 0\n",
    "            for data_dict in tqdm(data_loader, desc=\"Embedding cells\"):\n",
    "                input_gene_ids = data_dict[\"gene\"].to(device)\n",
    "                src_key_padding_mask = input_gene_ids.eq(\n",
    "                    vocab[model_configs[\"pad_token\"]]\n",
    "                )\n",
    "                print(\" input gene ids to device\")\n",
    "                embeddings = model._encode(\n",
    "                    input_gene_ids,\n",
    "                    data_dict[\"expr\"].to(device),\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=data_dict[\"batch_labels\"].to(device)\n",
    "                    if use_batch_labels\n",
    "                    else None,\n",
    "                )\n",
    "                print(\"encoded embeddings\")\n",
    "\n",
    "                embeddings = embeddings[:, 0, :]  # get the <cls> position embedding\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "                cell_embeddings[count : count + len(embeddings)] = embeddings\n",
    "                count += len(embeddings)\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cell embedding mode: {cell_embedding_mode}\")\n",
    "    return cell_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedd all data func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:14:44.544700800Z",
     "start_time": "2023-12-03T14:14:44.501571241Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def embed_data(\n",
    "    adata_or_file: Union[AnnData, PathLike],\n",
    "    model_dir: PathLike,\n",
    "    cell_type_key: str = \"cell_type\",\n",
    "    gene_col: str = \"feature_name\",\n",
    "    max_length=1200,\n",
    "    batch_size=64,\n",
    "    obs_to_save: Optional[list] = None,\n",
    "    device: Union[str, torch.device] = \"cuda\",\n",
    "    use_fast_transformer: bool = True,\n",
    "    return_new_adata: bool = False,\n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Preprocess anndata and embed the data using the model.\n",
    "\n",
    "    Args:\n",
    "        adata_or_file (Union[AnnData, PathLike]): The AnnData object or the path to the\n",
    "            AnnData object.\n",
    "        model_dir (PathLike): The path to the model directory.\n",
    "        cell_type_key (str): The key in adata.obs that contains the cell type labels.\n",
    "            Defaults to \"cell_type\".\n",
    "        gene_col (str): The column in adata.var that contains the gene names.\n",
    "        max_length (int): The maximum length of the input sequence. Defaults to 1200.\n",
    "        batch_size (int): The batch size for inference. Defaults to 64.\n",
    "        obs_to_save (Optional[list]): The list of obs columns to save in the output adata.\n",
    "            If None, will only keep the column of :attr:`cell_type_key`. Defaults to None.\n",
    "        device (Union[str, torch.device]): The device to use. Defaults to \"cuda\".\n",
    "        use_fast_transformer (bool): Whether to use flash-attn. Defaults to True.\n",
    "        return_new_adata (bool): Whether to return a new AnnData object. If False, will\n",
    "            add the cell embeddings to a new :attr:`adata.obsm` with key \"X_scGPT\".\n",
    "\n",
    "    Returns:\n",
    "        AnnData: The AnnData object with the cell embeddings.\n",
    "    \"\"\"\n",
    "    if isinstance(adata_or_file, AnnData):\n",
    "        adata = adata_or_file\n",
    "    else:\n",
    "        adata = sc.read_h5ad(adata_or_file)\n",
    "\n",
    "    # verify cell type key and gene col\n",
    "    assert cell_type_key in adata.obs\n",
    "    if gene_col == \"index\":\n",
    "        adata.var[\"index\"] = adata.var.index\n",
    "    else:\n",
    "        assert gene_col in adata.var\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"WARNING: CUDA is not available. Using CPU instead.\")\n",
    "\n",
    "    # LOAD MODEL\n",
    "    model_dir = Path(model_dir)\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "\n",
    "    # vocabulary\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        vocab[gene] if gene in vocab else -1 for gene in adata.var[gene_col]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "\n",
    "    # Binning will be applied after tokenization. A possible way to do is to use the unified way of binning in the data collator.\n",
    "\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    genes = adata.var[gene_col].tolist()\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # all_counts = adata.layers[\"counts\"]\n",
    "    # num_of_non_zero_genes = [\n",
    "    #     np.count_nonzero(all_counts[i]) for i in range(all_counts.shape[0])\n",
    "    # ]\n",
    "    # max_length = min(max_length, np.max(num_of_non_zero_genes) + 1)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        ntoken=len(vocab),\n",
    "        d_model=model_configs[\"embsize\"],\n",
    "        nhead=model_configs[\"nheads\"],\n",
    "        d_hid=model_configs[\"d_hid\"],\n",
    "        nlayers=model_configs[\"nlayers\"],\n",
    "        nlayers_cls=model_configs[\"n_layers_cls\"],\n",
    "        n_cls=1,\n",
    "        vocab=vocab,\n",
    "        dropout=model_configs[\"dropout\"],\n",
    "        pad_token=model_configs[\"pad_token\"],\n",
    "        pad_value=model_configs[\"pad_value\"],\n",
    "        do_mvc=True,\n",
    "        do_dab=False,\n",
    "        use_batch_labels=False,\n",
    "        domain_spec_batchnorm=False,\n",
    "        explicit_zero_prob=False,\n",
    "        use_fast_transformer=use_fast_transformer,\n",
    "        fast_transformer_backend=\"flash\",\n",
    "        pre_norm=False,\n",
    "    )\n",
    "    load_pretrained(model, torch.load(model_file), verbose=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"loaded model\")\n",
    "\n",
    "    # get cell embeddings\n",
    "    cell_embeddings = get_batch_cell_embeddings(\n",
    "        adata,\n",
    "        cell_embedding_mode=\"cls\",\n",
    "        model=model,\n",
    "        vocab=vocab,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size,\n",
    "        model_configs=model_configs,\n",
    "        gene_ids=gene_ids,\n",
    "        use_batch_labels=False,\n",
    "    )\n",
    "    print(\"got cell embeddings\")\n",
    "\n",
    "    if return_new_adata:\n",
    "        obs_to_save = [cell_type_key] if obs_to_save is None else obs_to_save\n",
    "        obs_df = adata.obs[obs_to_save]\n",
    "        return sc.AnnData(X=cell_embeddings, obs=obs_df, dtype=\"float32\")\n",
    "\n",
    "    adata.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check cuda available\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:15:49.703380194Z",
     "start_time": "2023-12-03T14:15:49.654687313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T14:14:47.423559835Z",
     "start_time": "2023-12-03T14:14:47.410553327Z"
    }
   },
   "outputs": [],
   "source": [
    "#print working directory:\n",
    "print(os.getcwd())\n",
    "#print working dir contents:\n",
    "print(os.listdir(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory:\n",
    "#os.chdir(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "base_dir = short_utils.get_base_dir()\n",
    "base_dir\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:15:02.262319948Z",
     "start_time": "2023-12-03T14:15:02.217895617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:30:49.485823402Z",
     "start_time": "2023-12-04T12:30:49.439347914Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# load data\n",
    "full_adata = sc.read_h5ad(base_dir / 'training_data/tcga/genexp_data/xena_pan_can_genexp_clin.h5ad')\n",
    "# load and examine the data at data/brca_scrna_epithelial.h5ad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#truncate the data to 1000 cells\n",
    "my_adata = full_adata[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:30:51.771216979Z",
     "start_time": "2023-12-04T12:30:51.756249888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#deal with na\n",
    "my_adata.obs['tumor_type'].isna().sum()\n",
    "#var = my_adata.var\n",
    "#var.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#replace nan with 'unknwon'\n",
    "\n",
    "col_name = 'tumor_type'\n",
    "\n",
    "# Add 'unknown' to the categories\n",
    "if 'unknown' not in my_adata.obs[col_name].cat.categories:\n",
    "    my_adata.obs[col_name] = my_adata.obs[col_name].cat.add_categories('unknown')\n",
    "\n",
    "# Now you can fill NaN values with 'unknown'\n",
    "my_adata.obs[col_name] = my_adata.obs[col_name].fillna('unknown')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "# prep args for embed:\n",
    "\n",
    "#if plot by label, set the cell type arg to the cool with label\n",
    "\n",
    "embed_args = {'adata_or_file': my_adata,\n",
    "              'model_dir': Path(base_dir / 'scgpt/models/scGPT_pancancer'),\n",
    "              'cell_type_key': \"tumor_type\",\n",
    "                'gene_col': \"hgnc_gene\",\n",
    "              'max_length' : 20000,\n",
    "              'batch_size' : 1,\n",
    "              'obs_to_save':  None,\n",
    "              'device':  \"cuda\",\n",
    "              'use_fast_transformer': False,\n",
    "              'return_new_adata':  True,\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T14:16:48.973278246Z",
     "start_time": "2023-12-03T14:16:48.928256256Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 17350/20530 genes in vocabulary of size 60697.\n",
      "loaded model\n",
      "loaded count matrix\n",
      "created dataset\n",
      "created collator\n",
      "created data loader\n",
      "created device\n",
      "created intial cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  10%|█         | 1/10 [00:00<00:08,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  20%|██        | 2/10 [00:01<00:07,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  30%|███       | 3/10 [00:02<00:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  40%|████      | 4/10 [00:03<00:05,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  50%|█████     | 5/10 [00:04<00:04,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  60%|██████    | 6/10 [00:05<00:03,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  70%|███████   | 7/10 [00:06<00:02,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  80%|████████  | 8/10 [00:07<00:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells:  90%|█████████ | 9/10 [00:08<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input gene ids to device\n",
      "encoded embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells: 100%|██████████| 10/10 [00:09<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "cell_embbed = embed_data(**embed_args)\n",
    "\n",
    "#clean cell output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "consider to avoid memory outage.\n",
    "figure out why reserving so much memory.\n",
    "https://github.com/rentruewang/koila"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#save the cell_embbed\n",
    "cell_embbed.obs = my_adata.obs.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = Path(base_dir / 'scgpt/data/bulk_brca_erbb2/tcga_all_scgpt_embb.h5ad')\n",
    "cell_embbed.write_h5ad(save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# add clinical data to adata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load clinical data \n",
    "clin_path = Path(base_dir / 'training_data/tcga/clinical/brca_tcga_pan_can_atlas_2018_clinical_data.tsv')\n",
    "\n",
    "clin_df = pd.read_csv(clin_path, sep='\\t', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:24:48.502065583Z",
     "start_time": "2023-12-04T12:24:48.461288727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clin_df.iloc[0:5,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:24:50.213034191Z",
     "start_time": "2023-12-04T12:24:50.177148505Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T11:35:42.142351875Z",
     "start_time": "2023-12-04T11:35:42.126959574Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = pd.Series(clin_df.columns)\n",
    "keep_cols = [0,1,2,3,5,6,19,21,28,29,30,33,34,36,38,40,41,46,49,50,51,56]\n",
    "#keep selected cols\n",
    "clin_df = clin_df.iloc[:,keep_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:24:56.149997520Z",
     "start_time": "2023-12-04T12:24:56.120344906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clin_df = clin_df.set_index(['Sample ID'])\n",
    "clin_df.index = clin_df.index.str.replace('-','.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:25:28.721709179Z",
     "start_time": "2023-12-04T12:25:28.679974884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#keep only samples in my_adata.obs['Sample_ID']\n",
    "clin_df = clin_df.loc[my_adata.obs['Sample_ID'],:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:25:50.984112320Z",
     "start_time": "2023-12-04T12:25:50.931857473Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#add clinical data to adata by joining on sample id\n",
    "new_obs = pd.merge(my_adata.obs.copy(), clin_df, left_on='Sample_ID', right_on='Sample ID', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:31:17.897283972Z",
     "start_time": "2023-12-04T12:31:17.874300933Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for each col, count na:\n",
    "for col in new_obs.columns:\n",
    "    print(col, new_obs[col].isna().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:31:47.512765520Z",
     "start_time": "2023-12-04T12:31:47.497965136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "new_obs = new_obs.drop(['Neoplasm Histologic Grade'], axis=1) if 'Neoplasm Histologic Grade' in new_obs.columns else new_obs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:33:20.097690886Z",
     "start_time": "2023-12-04T12:33:20.079951644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#add the new obs to cell em\n",
    "cell_embbed.obs = new_obs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:35:22.807180984Z",
     "start_time": "2023-12-04T12:35:22.762416072Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cell_embbed.write_h5ad(base_dir / 'scgpt/data/bulk_brca_erbb2/tcga_brca_erbb2_scgpt_emb_oncosig_sub_genes_clin.h5ad')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:49:13.583799051Z",
     "start_time": "2023-12-04T12:49:13.523505066Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#add new obs to my adata and make smaple id the index\n",
    "my_adata.obs = new_obs\n",
    "my_adata.obs.set_index('Sample_ID', inplace=True) \n",
    "#full_adata = sc.read_h5ad(base_dir / 'scgpt/data/bulk_brca_erbb2/tcga_brca_erbb2_oncosig_sub_genes.h5ad')\n",
    "# save my_adata as 'tcga_brca_erbb2_oncosig_sub_genes_clin.h5ad'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:41:57.365839692Z",
     "start_time": "2023-12-04T12:41:57.314976619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_adata.write_h5ad(base_dir / 'scgpt/data/bulk_brca_erbb2/tcga_brca_erbb2_oncosig_sub_genes_clin.h5ad')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T12:42:17.467528244Z",
     "start_time": "2023-12-04T12:42:17.403153704Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# plotting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#plot the result cell_embbed.X which is #num cell rows of 512 collums\n",
    "import plotly.express as px\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T12:50:09.550523927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_adata.X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "projection_data = my_adata.X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:44:54.997872231Z",
     "start_time": "2023-12-04T14:44:54.940057426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#fit the projection\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(projection_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:44:58.484122525Z",
     "start_time": "2023-12-04T14:44:56.388650056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "# create a PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Perform PCA on the embeddings\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(projection_data)\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PCA 1', 'PCA 2'])\n",
    "pca_df.index = my_adata.obs.index\n",
    "\n",
    "# If you have the obs data as a DataFrame named my_adata.obs, concatenate it with the PCA results\n",
    "full_df = pd.concat([my_adata.obs, pca_df], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:45:02.261496795Z",
     "start_time": "2023-12-04T14:45:02.191706489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Create a PCA plot\n",
    "fig = px.scatter(pca_df, x='PCA 1', y='PCA 2',color=my_adata.obs['tumor_type'], title=\"PCA Plot\")\n",
    "fig.update_layout(xaxis_title=\"PCA 1\", yaxis_title=\"PCA 2\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:45:05.325092904Z",
     "start_time": "2023-12-04T14:45:05.239008786Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#print explained variance ratio\n",
    "print('explained variance by pc 1 & 2: ',pca.explained_variance_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get the loading of all cols in"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Assuming my_adata.obs is a DataFrame containing the original observational data\n",
    "# Calculate correlations\n",
    "correlation_matrix = full_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:45:26.241167740Z",
     "start_time": "2023-12-04T14:45:26.199245947Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# select and analyze subsets based on umap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "# Prepare your data\n",
    "umap_x = embedding[:, 0]\n",
    "umap_y = embedding[:, 1]\n",
    "# Create a DataFrame for Plotly: add the UMAP cols to my_adata.obs\n",
    "umap_df = pd.DataFrame()\n",
    "umap_df['UMAP 1'] = umap_x\n",
    "umap_df['UMAP 2'] = umap_y\n",
    "umap_df.index = my_adata.obs.index\n",
    "#add the obs data to the umap df\n",
    "umap_df = pd.concat([my_adata.obs, umap_df], axis=1)\n",
    "\n",
    "plt_title = 'UMAP all tcga scgpt all genes'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:47:02.863246421Z",
     "start_time": "2023-12-04T14:47:02.773743569Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "plotyly themes:   ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n",
    "         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n",
    "         'ygridoff', 'gridon', 'none']\n",
    "https://plotly.com/python/templates/\n",
    "https://plotly.com/python/reference/layout/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "import plotly.io as pio\n",
    "# Create a Plotly figure\n",
    "fig = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', color='tumor_type',\n",
    "                 color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "                 title=plt_title,\n",
    "                 labels={'Label': 'cancer'},\n",
    "                 opacity=0.8)\n",
    "\n",
    "# Update layout for background and size nand use seaborn theme\n",
    "fig.update_layout(\n",
    "    template='seaborn',\n",
    "    width=1200,\n",
    "    height=1000,\n",
    "    legend_title_text='Cancer',\n",
    "    xaxis_title='UMAP 1',\n",
    "    yaxis_title='UMAP 2'\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust marker size and opacity\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.8))\n",
    "\n",
    "\n",
    "# Save the plot as an image file\n",
    "#pio.write_image(fig, 'umap_plot.png')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:47:34.023188278Z",
     "start_time": "2023-12-04T14:47:33.890019402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(umap_df.columns)\n",
    "umap_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#save the plot\n",
    "plots_dir = Path(base_dir / 'scgpt/plots')\n",
    "fig.write_image(str(plots_dir / f'{plt_title}.png'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get working dir\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# analyze results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "measure umap dffrentiation of groups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#cluster the umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure, mutual_info_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#clustering_vals = umap_df[['UMAP 1', 'UMAP 2']]\n",
    "clustering_vals = my_adata.X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dbscan\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5).fit(clustering_vals)\n",
    "clusters = clustering.labels_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#try other clustering methods\n",
    "import hdbscan\n",
    "\n",
    "# Fit the HDBSCAN model\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10)\n",
    "clusters = clusterer.fit_predict(clustering_vals)\n",
    "\n",
    "# Adjust parameters like `min_cluster_size` and `min_samples` based on your dat\n",
    "\n",
    "#print number of clusters\n",
    "print('number of clusters: ', len(set(clusters)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Fit the GMM model\n",
    "gmm = GaussianMixture(n_components=21, covariance_type='full')\n",
    "clusters = gmm.fit_predict(clustering_vals)\n",
    "\n",
    "# Adjust `n_components` (number of clusters) and `covariance_type` based on data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=21, init='k-means++')\n",
    "clusters = kmeans.fit_predict(clustering_vals)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "umap_df['cluster'] = clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "# Create a Plotly figure\n",
    "fig = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', color='cluster',\n",
    "                 color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "                 title=plt_title,\n",
    "                 labels={'Label': 'cancer'},\n",
    "                 opacity=0.8)\n",
    "\n",
    "# Update layout for background and size nand use seaborn theme\n",
    "fig.update_layout(\n",
    "    template='seaborn',\n",
    "    width=600,\n",
    "    height=500,\n",
    "    legend_title_text='Cancer',\n",
    "    xaxis_title='UMAP 1',\n",
    "    yaxis_title='UMAP 2'\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust marker size and opacity\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.8))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subtype_counts = umap_df.groupby(['cluster', 'tumor_type']).size().unstack(fill_value=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#subtype metric\n",
    "#measure accuracy by checking if subtypes tended to be in homogeneous clusters\n",
    "\n",
    "# Initialize accuracy dictionary with tumor types as keys\n",
    "accuracy_dict = {tumor_type: 0 for tumor_type in umap_df['tumor_type'].unique()}\n",
    "\n",
    "# Initialize counts dictionary with tumor types as keys and their counts as values\n",
    "counts_dict = umap_df['tumor_type'].value_counts().to_dict()\n",
    "\n",
    "# Iterate over each cluster\n",
    "for cluster in umap_df['cluster'].unique():\n",
    "    # Get the subset of the DataFrame that corresponds to the current cluster\n",
    "    cluster_data = umap_df[umap_df['cluster'] == cluster]\n",
    "\n",
    "    # Count the number of occurrences of each tumor type in this cluster\n",
    "    type_counts_in_cluster = cluster_data['tumor_type'].value_counts()\n",
    "\n",
    "    # Calculate the clustering accuracy for each tumor type\n",
    "    for tumor_type, count_in_cluster in type_counts_in_cluster.items():\n",
    "        # Proportion of samples in the cluster of this subtype\n",
    "        accuracy_in_cluster = count_in_cluster / cluster_data.shape[0]\n",
    "\n",
    "        # Proportion of total samples of this subtype in the cluster\n",
    "        prop_of_subtype_in_cluster = count_in_cluster / counts_dict[tumor_type]\n",
    "\n",
    "        # Multiply and add to the accuracy dictionary for the tumor type\n",
    "        accuracy_dict[tumor_type] += accuracy_in_cluster * prop_of_subtype_in_cluster\n",
    "#sort the dict by values desc\n",
    "#accuracy_dict = dict(sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# create a df of tumor type and accuracy\n",
    "accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index', columns=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the weighted average isolation accuracy\n",
    "weighted_avg_isolation = sum(accuracy_dict[tumor_type] * counts_dict[tumor_type] for tumor_type in accuracy_dict) / (umap_df.shape[0])\n",
    "\n",
    "\n",
    "# Print the weighted average isolation accuracy\n",
    "print(f'Weighted average isolation accuracy: {weighted_avg_isolation:.4f}')\n",
    "#print the df\n",
    "accuracy_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "#cluster metric\n",
    "#measure entropy of each cluster\n",
    "entropy_scores = {}\n",
    "\n",
    "# Initialize the sum of weighted entropies and total number of samples\n",
    "weighted_entropy_sum = 0\n",
    "total_samples = umap_df.shape[0]\n",
    "\n",
    "# Iterate over all unique clusters and measure the entropy of the tumor_type distribution\n",
    "for cluster in umap_df['cluster'].unique():\n",
    "    # Get the tumor_type distribution of the cluster\n",
    "    cluster_distribution = subtype_counts.loc[cluster]\n",
    "\n",
    "    # Calculate the entropy for the cluster's tumor_type distribution\n",
    "    # Convert the counts to a probability distribution (summing to 1)\n",
    "    cluster_prob_dist = cluster_distribution / cluster_distribution.sum()\n",
    "    cluster_entropy = entropy(cluster_prob_dist)\n",
    "\n",
    "    # Store the entropy score for the cluster\n",
    "    entropy_scores[cluster] = cluster_entropy\n",
    "\n",
    "    #measure total entropy\n",
    "    cluster_size = umap_df[umap_df['cluster'] == cluster].shape[0]\n",
    "    weighted_entropy_sum += entropy_scores[cluster] * cluster_size\n",
    "\n",
    "\n",
    "# Print the entropy scores\n",
    "#for cluster, e_score in entropy_scores.items():\n",
    "#    print(f'Entropy for cluster {cluster}: {e_score:.4f}')\n",
    "\n",
    "\n",
    "# Calculate the weighted average entropy across all clusters\n",
    "weighted_avg_entropy = weighted_entropy_sum / total_samples\n",
    "\n",
    "# Print the weighted average entropy\n",
    "#print(f'Weighted average entropy across all clusters: {weighted_avg_entropy:.4f}')\n",
    "\n",
    "\n",
    "# Calculate the mutual information between the cluster labels for the entire dataset and the true labels\n",
    "mi_score = mutual_info_score(umap_df['tumor_type'], umap_df['cluster'])\n",
    "#print(f'Mutual Information for the entire clustering: {mi_score}')\n",
    "\n",
    "#create a table with the avg entorpy and mi score where the metrics are the rows\n",
    "metrics_df = pd.DataFrame({'avg_iso_w' : [weighted_avg_isolation],'weighted_avg_entropy': [weighted_avg_entropy], 'mi_score': [mi_score]}).T\n",
    "metrics_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_completeness_v_measure\n",
    "\n",
    "# Assuming umap_df contains your data, with 'tumor_type' as true labels and 'cluster' as predicted labels\n",
    "true_labels = umap_df['tumor_type']\n",
    "predicted_labels = umap_df['cluster']\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette = silhouette_score(clustering_vals, predicted_labels)\n",
    "\n",
    "# Fowlkes-Mallows Index\n",
    "fmi = fowlkes_mallows_score(true_labels, predicted_labels)\n",
    "\n",
    "# Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "# Normalized Mutual Information\n",
    "nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "\n",
    "# Homogeneity, Completeness, V-Measure\n",
    "homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(true_labels, predicted_labels)\n",
    "\n",
    "# Purity is not directly available in sklearn, but can be calculated manually\n",
    "\n",
    "# Calculate purity\n",
    "def purity_score(y_true, y_pred):\n",
    "    # Compute contingency matrix\n",
    "    matrix = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(matrix, axis=0)) / np.sum(matrix)\n",
    "\n",
    "# Assuming umap_df contains your data, with 'tumor_type' as true labels and 'cluster' as predicted labels\n",
    "true_labels = umap_df['tumor_type']\n",
    "predicted_labels = umap_df['cluster']\n",
    "\n",
    "purity = purity_score(true_labels, predicted_labels)\n",
    "#print(f'Purity: {purity}')\n",
    "\n",
    "\n",
    "#add each metric as a row to the metrics df\n",
    "metrics_df = pd.concat([metrics_df, pd.DataFrame({'silhouette': [silhouette], 'fmi': [fmi], 'ari': [ari], 'nmi': [nmi], 'homogeneity': [homogeneity], 'completeness': [completeness], 'v_measure': [v_measure], 'purity': [purity]}).T])\n",
    "metrics_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a table of all the metrics as well as weighted avg entropy and mi score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "subset data with umap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = umap_df.corr()\n",
    "correlation_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:48:02.291005710Z",
     "start_time": "2023-12-04T14:48:02.209868981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define grid subset: umap 1 min, max; umap 2 min, max:\n",
    "box_select = {'UMAP 1': [0, 12], 'UMAP 2': [5, 9]}\n",
    "# select from umap df:\n",
    "umap_subset = umap_df.loc[(umap_df['UMAP 1'] > box_select['UMAP 1'][0]) & (umap_df['UMAP 1'] <= box_select['UMAP 1'][1]) & (umap_df['UMAP 2'] > box_select['UMAP 2'][0]) & (umap_df['UMAP 2'] <= box_select['UMAP 2'][1]), :]\n",
    "#select the opposite as well\n",
    "umap_subset_opp = umap_df.loc[(umap_df['UMAP 1'] <= box_select['UMAP 1'][0]) | (umap_df['UMAP 1'] > box_select['UMAP 1'][1]) | (umap_df['UMAP 2'] <= box_select['UMAP 2'][0]) | (umap_df['UMAP 2'] > box_select['UMAP 2'][1]), :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:06:20.177779552Z",
     "start_time": "2023-12-04T14:06:19.778309644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print data type for each collumn\n",
    "for col in umap_subset.columns:\n",
    "    print(col, umap_subset[col].dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:52.926767061Z",
     "start_time": "2023-12-04T13:11:52.512184089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "#create a table of the mean of the numerical cols in each df as well as the result of a \n",
    "# two tailed t test\n",
    "\n",
    "# Identify numerical columns (excluding 'UMAP 1' and 'UMAP 2' as they were used for subsetting)\n",
    "numerical_cols = umap_df.select_dtypes(include='number').columns.drop(['UMAP 1', 'UMAP 2'])\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Column', 'Mean Subset', 'Mean Opposite Subset', 'T-test P-value'])\n",
    "\n",
    "# Loop through each numerical column to compute means and perform t-tests\n",
    "for col in numerical_cols:\n",
    "    mean_subset = umap_subset[col].mean()\n",
    "    mean_subset_opp = umap_subset_opp[col].mean()\n",
    "\n",
    "    # Perform two-tailed t-test\n",
    "    t_test_result = ttest_ind(umap_subset[col], umap_subset_opp[col], nan_policy='omit')\n",
    "\n",
    "    # Append results to the DataFrame\n",
    "    results_df = results_df.append({\n",
    "        'Column': col,\n",
    "        'Mean Subset': mean_subset,\n",
    "        'Mean Opposite Subset': mean_subset_opp,\n",
    "        'T-test P-value': t_test_result.pvalue\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:06:23.434123865Z",
     "start_time": "2023-12-04T14:06:23.354645534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = umap_df.select_dtypes(include='category').columns\n",
    "categorical_cols"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:21:26.220766382Z",
     "start_time": "2023-12-04T13:21:25.882220691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# First, create a DataFrame without the columns you want to exclude\n",
    "reduced_df = umap_df.drop(['UMAP 1', 'UMAP 2', 'cell_type', 'Sex'], axis=1)\n",
    "\n",
    "# Now, select only the categorical columns from this reduced DataFrame\n",
    "categorical_cols = reduced_df.select_dtypes(include='category').columns\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Column', 'Counts in Subset', 'Counts in Opposite Subset', 'Chi-Squared P-value'])\n",
    "\n",
    "# Loop through each categorical column to count levels and perform chi-squared tests\n",
    "for col in categorical_cols:\n",
    "    # Count the levels in each subset\n",
    "    counts_subset = umap_subset[col].value_counts()\n",
    "    counts_subset_opp = umap_subset_opp[col].value_counts()\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.DataFrame({\n",
    "        'Subset': counts_subset,\n",
    "        'Opposite Subset': counts_subset_opp\n",
    "    }).fillna(0)\n",
    "\n",
    "    # Perform chi-squared test\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Append results to the DataFrame\n",
    "    results_df = results_df.append({\n",
    "        'Column': col,\n",
    "        'Counts in Subset': dict(counts_subset),\n",
    "        'Counts in Opposite Subset': dict(counts_subset_opp),\n",
    "        'Chi-Squared P-value': p\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:07:41.460099255Z",
     "start_time": "2023-12-04T14:07:41.384184746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "\n",
    "\n",
    "# Extracting the relevant data from results_df\n",
    "subtype_data = results_df.loc[results_df['Column'] == 'Subtype'].iloc[0]\n",
    "counts_subset = subtype_data['Counts in Subset']\n",
    "counts_subset_opp = subtype_data['Counts in Opposite Subset']\n",
    "chi_squared_p_value = subtype_data['Chi-Squared P-value']\n",
    "\n",
    "# Getting the union of keys from both count dictionaries\n",
    "all_subtypes = set(counts_subset.keys()).union(set(counts_subset_opp.keys()))\n",
    "\n",
    "# Creating a new DataFrame\n",
    "subtype_counts_df = pd.DataFrame(index=all_subtypes, columns=['Subset', 'Opposite Subset'])\n",
    "\n",
    "# Populating the DataFrame\n",
    "for subtype in all_subtypes:\n",
    "    subtype_counts_df.loc[subtype, 'Subset'] = counts_subset.get(subtype, 0)\n",
    "    subtype_counts_df.loc[subtype, 'Opposite Subset'] = counts_subset_opp.get(subtype, 0)\n",
    "\n",
    "# Adding the chi-squared p-value as a new column\n",
    "subtype_counts_df['Chi-Squared P-value'] = chi_squared_p_value\n",
    "\n",
    "# Total number of samples in each subset\n",
    "n_subset = umap_subset.shape[0]\n",
    "n_subset_opp = umap_subset_opp.shape[0]\n",
    "total_samples = n_subset + n_subset_opp\n",
    "\n",
    "# Add a new column for the probability calculation\n",
    "subtype_counts_df['Random Selection Probability'] = 0\n",
    "\n",
    "for subtype in all_subtypes:\n",
    "    # Total number of samples of this subtype in the entire dataset\n",
    "    total_subtype_samples = counts_subset.get(subtype, 0) + counts_subset_opp.get(subtype, 0)\n",
    "\n",
    "    # Number of samples of this subtype in the umap_subset\n",
    "    subtype_samples_in_subset = counts_subset.get(subtype, 0)\n",
    "\n",
    "    # Calculate the probability using the hypergeometric distribution\n",
    "    p_value = hypergeom(total_samples, total_subtype_samples, n_subset).pmf(subtype_samples_in_subset)\n",
    "    subtype_counts_df.loc[subtype, 'Random Selection Probability'] = p_value\n",
    "\n",
    "# Display the new DataFrame\n",
    "subtype_counts_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:07:48.642965875Z",
     "start_time": "2023-12-04T14:07:48.597754045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cell_embbed.obs.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Set style for a light background\n",
    "plt.style.use('seaborn-v0_8-pastel')  # or 'classic'\n",
    "\n",
    "#create plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Use the 'oncosig_label' values for coloring the points\n",
    "# 'cmap' can be adjusted to your preferred color map\n",
    "\n",
    "# Manually set colors based on 'oncosig_label_ERBB2'\n",
    "colors = ['darkblue' if label == 0 else 'red' for label in my_adata.obs['oncosig_label_ERBB2']]\n",
    "\n",
    "\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=colors, s=8)\n",
    "plt_title = 'UMAP tcga brca scgpt emb sub genes & onc label'\n",
    "plt.title(plt_title, fontsize=18)\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "\n",
    "\n",
    "# Create a legend\n",
    "legend_labels = ['Label 0', 'Label 1']\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=legend_labels[i],\n",
    "                              markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=10)\n",
    "                   for i in range(2)]\n",
    "plt.legend(handles=legend_elements, title='Oncosig Labels')\n",
    "\n",
    "# Save the plot to a file\n",
    "\n",
    "#plt.savefig(f'plots/{plt_title}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot in the notebook\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "! compare to raw all genes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create plot\n",
    "raw_all_genes_embedding = reducer.fit_transform(all_genes_adata.X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 'cmap' can be adjusted to your preferred color map\n",
    "\n",
    "\n",
    "# Manually set colors based on 'oncosig_label_ERBB2'\n",
    "colors = ['darkblue' if label == 0 else 'red' for label in cell_embbed.obs['oncosig_label_ERBB2']]\n",
    "\n",
    "\n",
    "\n",
    "scatter = plt.scatter(raw_all_genes_embedding[:, 0], raw_all_genes_embedding[:, 1], c=colors, s=7)\n",
    "plt_title = 'UMAP tcga brca raw all genes emb + onc label'\n",
    "\n",
    "plt.title(plt_title, fontsize=18)\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "\n",
    "# Create a legend\n",
    "legend_labels = ['Label 0', 'Label 1']\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=legend_labels[i],\n",
    "                              markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=10)\n",
    "                   for i in range(2)]\n",
    "plt.legend(handles=legend_elements, title='Oncosig Labels')\n",
    "# Save the plot to a file\n",
    "\n",
    "plt.savefig(f'plots/{plt_title}.png', dpi=300, bbox_inches='tight')\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "! compare to umap of maually selected genes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create plot\n",
    "raw_selected_embedding = reducer.fit_transform(my_adata.X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "\n",
    "\n",
    "# Manually set colors based on 'oncosig_label_ERBB2'\n",
    "colors = ['darkblue' if label == 0 else 'red' for label in cell_embbed.obs['oncosig_label_ERBB2']]\n",
    "\n",
    "\n",
    "\n",
    "scatter = plt.scatter(raw_selected_embedding[:, 0], raw_selected_embedding[:, 1], c=colors, s=7)\n",
    "plt_title = 'UMAP tcga brca raw selected genes emb + onc label'\n",
    "\n",
    "plt.title(plt_title, fontsize=18)\n",
    "plt.xlabel('UMAP 1', fontsize=12)\n",
    "plt.ylabel('UMAP 2', fontsize=12)\n",
    "\n",
    "# Create a legend\n",
    "legend_labels = ['Label 0', 'Label 1']\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=legend_labels[i],\n",
    "                              markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=10)\n",
    "                   for i in range(2)]\n",
    "plt.legend(handles=legend_elements, title='Oncosig Labels')\n",
    "\n",
    "\n",
    "# Save the plot to a file\n",
    "\n",
    "plt.savefig(f'plots/{plt_title}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cell_embbed.write_h5ad('data/brca_scrna_epithelial_scGPT.h5ad')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# check corr between X and obs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#drop some of the meta data cols\n",
    "cell_embbed.obs.columns\n",
    "cols_drop = [ 'cell_type',\n",
    "       'Neoplasm Disease Stage American Joint Committee on Cancer Code',\n",
    "             'MSI MANTIS Score', 'MSIsensor Score', 'Mutation Count',\n",
    " 'Overall Survival Status',\n",
    "       'American Joint Committee on Cancer Metastasis Stage Code',\n",
    "       'American Joint Committee on Cancer Tumor Stage Code',\n",
    "     'Progression Free Status',\n",
    "   'Sex', 'Somatic Status', ]\n",
    "cell_embbed.obs = cell_embbed.obs.drop(cols_drop, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get data frames and scale\n",
    "\n",
    "\n",
    "# Convert the X matrix to a DataFrame\n",
    "values_df = pd.DataFrame(cell_embbed.X, columns=[f'X_{i}' for i in range(cell_embbed.X.shape[1])])\n",
    "\n",
    "metadata = pd.DataFrame(cell_embbed.obs.copy().drop(['Patient ID'], axis=1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get numerical cols as num_metadata\n",
    "num_meta_cols = metadata.select_dtypes(include='number').columns\n",
    "\n",
    "#get cols of values\n",
    "\n",
    "#get a list of strings: Dim_0, Dim_2, until Dim_511\n",
    "val_cols = values_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scale\n",
    "#transform the vals df to log p 1\n",
    "values_df = np.log1p(values_df)\n",
    "#scale the vals df 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "values_df = scaler.fit_transform(values_df)\n",
    "#scale the numerical values of the metadata\n",
    "metadata[num_meta_cols] = scaler.fit_transform(metadata[num_meta_cols])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate null MI with bootstrapping\n",
    "null_mi_distribution = []\n",
    "#take a number of bootsraps ~ TWICE NUMBER OF COLUMNS\n",
    "\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# set threshold of MI\n",
    "mi_threshold = 0.95\n",
    "#choose 3 random test runs on which to check the data\n",
    "test_runs = np.random.randint(0, n_bootstraps, size=3)\n",
    "test_runs= np.append(test_runs, 0)\n",
    "test_runs\n",
    "\n",
    "# Flatten the DataFrame into a 1D NumPy array\n",
    "flattened_values = values_df.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find null mi dist using boot strap.\n",
    "for i in range(n_bootstraps):\n",
    "    # Randomly draw values from the flattened array\n",
    "    resampled_values = np.random.choice(flattened_values, size=len(values_df))\n",
    "\n",
    "    # Randomly select a column from metadata\n",
    "    random_meta_col = np.random.choice(metadata[num_meta_cols].columns)\n",
    "    selected_metadata = metadata[random_meta_col]\n",
    "\n",
    "    # Handle NA values - drop rows with NA in either column\n",
    "    na_mask =  selected_metadata.isna()\n",
    "    selected_values = resampled_values[~na_mask]\n",
    "    selected_metadata = selected_metadata[~na_mask]\n",
    "\n",
    "    #sanity check\n",
    "    # if i in test_runs:\n",
    "    #     #print length of bothg cols\n",
    "    #     print('selected values len:', len(selected_values))\n",
    "    #     print('selected metadata len:', len(selected_metadata))\n",
    "    #     print('shapes: val, meta: \\n' , selected_values.shape, selected_metadata.shape)\n",
    "    #     #print the first 5 rows of both cols\n",
    "    #     print('selected values 1:5:', selected_values[:5])\n",
    "    #     print('selected metadata 1:5:', selected_metadata[:5])\n",
    "\n",
    "    # If the metadata column is categorical, encode it\n",
    "    if selected_metadata.dtype == 'object' or selected_metadata.dtype.name == 'category':\n",
    "        encoder = LabelEncoder()\n",
    "        selected_metadata = encoder.fit_transform(selected_metadata)\n",
    "\n",
    "\n",
    "    # Calculate MI with the resampled data\n",
    "    mi = mutual_info_score(selected_values, selected_metadata)\n",
    "    null_mi_distribution.append(mi)\n",
    "\n",
    "# sort, and print the MI value at threshold percentile. do not rescale!\n",
    "null_mi_distribution = np.sort(null_mi_distribution)\n",
    "mi_threshold_val = null_mi_distribution[int(mi_threshold * len(null_mi_distribution))]\n",
    "print('MI threshold:', mi_threshold_val)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_metadata = metadata['Subtype']\n",
    "\n",
    "# Handle NA values - drop rows with NA in either column\n",
    "na_mask =  selected_metadata.isna()\n",
    "\n",
    "selected_metadata = selected_metadata[~na_mask]\n",
    "  # If the metadata column is categorical, encode it\n",
    "if selected_metadata.dtype == 'object' or selected_metadata.dtype.name == 'category':\n",
    "    encoder = LabelEncoder()\n",
    "    selected_metadata = encoder.fit_transform(selected_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "curr_vals = values_df[:,10]\n",
    "curr_vals = curr_vals[~na_mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Calculate MI with the resampled data\n",
    "actual_mi = mutual_info_score(selected_values, selected_metadata)\n",
    "actual_mi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#check if any na in val df numpy arr\n",
    "np.isnan(values_df).any()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#init a db: val_col, mi, p_val\n",
    "mi_df = pd.DataFrame(columns=['val_col', 'mi', 'p_val'])\n",
    "\n",
    "selected_metadata = metadata['Buffa Hypoxia Score']\n",
    "\n",
    "# Handle NA values - drop rows with NA in either column\n",
    "na_mask =  selected_metadata.isna()\n",
    "\n",
    "selected_metadata = selected_metadata[~na_mask]\n",
    "  # If the metadata column is categorical, encode it\n",
    "if selected_metadata.dtype == 'object' or selected_metadata.dtype.name == 'category':\n",
    "    encoder = LabelEncoder()\n",
    "    selected_metadata = encoder.fit_transform(selected_metadata)\n",
    "\n",
    "for val_col in range(values_df.shape[1]):\n",
    "    curr_vals = values_df[:,val_col]\n",
    "    curr_vals = curr_vals[~na_mask]\n",
    "\n",
    "    # Calculate MI with the resampled data\n",
    "    actual_mi = mutual_info_score(curr_vals, selected_metadata)\n",
    "\n",
    "    #check mi above threshold\n",
    "    if actual_mi > mi_threshold_val:\n",
    "        #calc p val:\n",
    "        print('actual mi:', actual_mi)\n",
    "        # Calculate the percentile of the actual MI value\n",
    "        percentile = np.sum(null_mi_distribution <= actual_mi) / len(null_mi_distribution)\n",
    "        p_value = 1 - percentile\n",
    "        #add to df\n",
    "    mi_df = mi_df.append({'val_col': val_col, 'mi': actual_mi, 'p_val': p_value}, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calc MI between each col in metadata and each col in values\n",
    "# create a dict whose keys are the meta data cols, and the values are lists of tuples of (col, mi, p-val) of cols whose mi was above the threshold\n",
    "mi_dict = {}\n",
    "for meta_col in metadata.columns:\n",
    "    selected_metadata = metadata[meta_col]\n",
    "\n",
    "    # Handle NA values - drop rows with NA in either column\n",
    "    na_mask =  selected_metadata.isna()\n",
    "\n",
    "    selected_metadata = selected_metadata[~na_mask]\n",
    "      # If the metadata column is categorical, encode it\n",
    "    if selected_metadata.dtype == 'object' or selected_metadata.dtype.name == 'category':\n",
    "        encoder = LabelEncoder()\n",
    "        selected_metadata = encoder.fit_transform(selected_metadata)\n",
    "\n",
    "    for val_col in range(values_df.shape[1]):\n",
    "        curr_vals = values_df[:,val_col]\n",
    "        curr_vals = curr_vals[~na_mask]\n",
    "\n",
    "        # Calculate MI with the resampled data\n",
    "        actual_mi = mutual_info_score(curr_vals, selected_metadata)\n",
    "\n",
    "        #check mi above threshold\n",
    "        if actual_mi > mi_threshold_val:\n",
    "            #calc p val:\n",
    "            print('actual mi:', actual_mi)\n",
    "            # Calculate the percentile of the actual MI value\n",
    "            percentile = np.sum(null_mi_distribution <= actual_mi) / len(null_mi_distribution)\n",
    "            p_value = 1 - percentile\n",
    "            #add to dict\n",
    "            if meta_col in mi_dict.keys():\n",
    "                mi_dict[meta_col].append((val_col, actual_mi, p_value))\n",
    "            else:\n",
    "                mi_dict[meta_col] = [(val_col, actual_mi, p_value)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find the cols with highest MI for each col in metadata\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get corrs for scaled data\n",
    "\n",
    "# Identify numerical columns (excluding 'UMAP 1' and 'UMAP 2' as they were used for subsetting)\n",
    "numerical_cols = cell_embbed.obs.select_dtypes(include='number')\n",
    "# Initialize a DataFrame to store the highest correlation for each numerical column\n",
    "highest_correlations = pd.DataFrame(columns=['obs_column', 'X_column', 'correlation'])\n",
    "\n",
    "\n",
    "# Loop through each column in the obs matrix\n",
    "for obs_col in metadata.columns:\n",
    "    #print('obs col 1[1]:', cell_embbed.obs[obs_col][0])\n",
    "    # Initialize a variable to store the highest correlation for this obs column\n",
    "    highest_corr = {'obs_column': obs_col, 'X_column': None, 'correlation': 0}\n",
    "\n",
    "\n",
    "\n",
    "      # If the metadata column is categorical, encode it\n",
    "    if selected_metadata.dtype == 'object' or selected_metadata.dtype.name == 'category':\n",
    "        #move to next col\n",
    "        continue\n",
    "    # Loop through each column in the X matrix\n",
    "    for val_col in range(values_df.shape[1]):\n",
    "        curr_vals = values_df[:,val_col]\n",
    "        #convert curr vals to pd series\n",
    "        curr_vals = pd.Series(curr_vals)\n",
    "        corr = cell_embbed.obs[obs_col].corr(curr_vals)\n",
    "        print('corr:', corr)\n",
    "        # Check if this is the highest correlation so far for this obs column\n",
    "        if abs(corr) > abs(highest_corr['correlation']):\n",
    "            highest_corr['X_column'] = val_col\n",
    "            highest_corr['correlation'] = corr\n",
    "\n",
    "    print(highest_corr)\n",
    "    # Append the highest correlation for this obs column to the DataFrame\n",
    "    highest_correlations = highest_correlations.append(highest_corr, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "highest_correlations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify numerical columns (excluding 'UMAP 1' and 'UMAP 2' as they were used for subsetting)\n",
    "numerical_cols = cell_embbed.obs.select_dtypes(include='number')\n",
    "# Initialize a DataFrame to store the highest correlation for each numerical column\n",
    "highest_correlations = pd.DataFrame(columns=['obs_column', 'X_column', 'correlation'])\n",
    "\n",
    "\n",
    "# Loop through each column in the obs matrix\n",
    "for obs_col in metadata.columns:\n",
    "    #print('obs col 1[1]:', cell_embbed.obs[obs_col][0])\n",
    "    # Initialize a variable to store the highest correlation for this obs column\n",
    "    highest_corr = {'obs_column': obs_col, 'X_column': None, 'correlation': 0}\n",
    "\n",
    "    # Loop through each column in the X matrix\n",
    "    for x_col in X_df.columns:\n",
    "        #print('x col 1[1]:', X_df[x_col][0])\n",
    "        # Compute the correlation\n",
    "        corr = cell_embbed.obs[obs_col].corr(X_df[x_col])\n",
    "        #print('corr:', corr)\n",
    "        # Check if this is the highest correlation so far for this obs column\n",
    "        if abs(corr) > abs(highest_corr['correlation']):\n",
    "            highest_corr['X_column'] = x_col\n",
    "            highest_corr['correlation'] = corr\n",
    "\n",
    "    print(highest_corr)\n",
    "    # Append the highest correlation for this obs column to the DataFrame\n",
    "    highest_correlations = highest_correlations.append(highest_corr, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "highest_correlations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# and now with MI:\n",
    "\n",
    "\n",
    "# Initialize a DataFrame to store the highest mutual information for each numerical column\n",
    "highest_mutual_info = pd.DataFrame(columns=['obs_column', 'X_column', 'mutual_info'])\n",
    "\n",
    "# Loop through each column in the obs matrix\n",
    "for obs_col in numerical_cols.columns:\n",
    "    # Initialize a variable to store the highest mutual information for this obs column\n",
    "    highest_mi = {'obs_column': obs_col, 'X_column': None, 'mutual_info': 0}\n",
    "\n",
    "    # Select and process the obs column, drop NA values\n",
    "    curr_obs = cell_embbed.obs[obs_col].dropna()\n",
    "    print('curr obs:', obs_col, len(curr_obs))\n",
    "\n",
    "    # Loop through each column in the X matrix\n",
    "    for x_col in X_df.columns:\n",
    "        # Compute the mutual information\n",
    "         # Select the x_col values, aligning with curr_obs by index\n",
    "        x_col_vals = X_df.loc[curr_obs.index, x_col]\n",
    "\n",
    "        mi = mutual_info_score(curr_obs, x_col_vals)\n",
    "\n",
    "\n",
    "        # Check if this is the highest mutual information so far for this obs column\n",
    "        if mi > highest_mi['mutual_info']:\n",
    "            highest_mi['X_column'] = x_col\n",
    "            highest_mi['mutual_info'] = mi\n",
    "\n",
    "    # Append the highest mutual information for this obs column to the DataFrame\n",
    "    highest_mutual_info = highest_mutual_info.append(highest_mi, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(highest_mutual_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# export notebook as script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nb_path = Path(base_dir / 'scgpt/j_scgpt_utils/cell_emb_plot.ipynb')\n",
    "output_path = Path(Path(base_dir / 'scgpt/j_scgpt_utils/cell_emb_plot.py'))\n",
    "short_utils.export_marked_cells(nb_path, output_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
